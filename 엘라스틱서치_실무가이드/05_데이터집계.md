# 데이터 집계

- 통계지표를 위해 엘라스틱서치에너느 Aggregation 기능 제공(group by와 비슷)
- 유료 패키지인 X-Pack 이용시 더욱더 간단한 Ansi SQL 구문으로 질의 가능

## 5.1 집계

- 다양한 문법으로 사용 가능하다

### 5.1.1 엘라스틱서치와 데이터 분석

- 보통 통계분석프로그램은 실시간이 아니라 배치 방식으로 데이터를 처리
- 엘라스틱서치는 많은양의 데이터를 조각내어 관리하므로 문서의 수가 늘어나도 배치 처리보다 좀 더 실시간에 가깝게 문서를 처리할 수 있다.
- SQL과 엘라스틱서치와의 차이를 비교한다
- SQL로는 Group by 구문을 통해 집계

~~~
SELECT SUM(ratings) FROM MOVIE_REVIEW GROUP BY MOVIE_NO;
~~~

- 엘라스틱서치의 Query DSL로 집계하는 쿼리의 경우

~~~
{
    "aggs" : {
        "movie_no_agg" : {
            "terms" : {
                "field" : "movie_no" 
            },
            "aggs" : { 
                "ratings_agg" : {
                    "sum" : {
                        "field" : "ratings" 
                    }
                }
            }
        }
    }
}
~~~

- 엘라스틱서치는 인덱스를 활용해 분산 처리가 가능하기 때문에 SQL보다 더 많은 데이터를 빠르게 집계 가능

### 5.1.2 엘라스틱서치가 집계에 사용하는 기술
- 캐시
  - 집계 쿼리로 값을 조회하면 엘라스틱서치의 마스터 노드가 여러 노드에 있는 데이터를 집계해 질의에 답변한다.
  - 데이터에 양이 클수록 집계에는 많은 양의 CPU와 메모리 자원이 소모되고 질의 응답 시간 길어짐 
  - 노드의 하드웨어 성능을 높여 해결 가능하지만 돈 많이 필요하다
  - 효율적인 해결책은 엘라스틱서치의 '캐시' 이용
  - 캐시는 질의의 결과를 버퍼에 두고, 같은 질의에 대해 보관된 결과 반환
  - 일반적으로 힙 메모리의 1%이나, elasticsearch.yml파일에서 수정 가능
  - 엘라스틱서치는 Node query Cache, Shard request Cache, Field data Cache  지원
~~~
indicies.requests.cache.size: 2%
~~~
- Node query Cache
  - LRU 캐시. 캐시 용량이 가득 차면 사용량이 가장 적은 데이터 삭제. 
  - 기본적으로 10%의 필터 캐시가 메모리를 제어. elasticsearch.yml파일에서 수정 가능
~~~
index.queries.cache.enabled:true
~~~

- Shard request Cache
  - 엘라스틱서치는 인덱스의 수평적 확산과 병렬 처리를 통한 성능 향상을 위해 고안된 개념 
  - 샤드는 데이터를 분산 저장하기 위한 단위로서, 그 자체가 온전한 기능을 가진 독립 인덱스
  - Shard request Cache는 이 샤드에서 수행된 쿼리의 결과를 캐싱
  - 샤드의 내용이 변경되면 캐시가 삭제되기 때문에 업데이트가 빈번한 인덱스에서는 성능 저하

- Field data Cache
  - 엘라스틱서치가 필드에서 집계 연산을 수행할 때는 모든 필드 값을 메모리에 로드한다.
  - 이러한 이유로 엘라스틱서치에서 계산되는 집계 쿼리는 성능적인 측면에서 비용 증가.
  - Field data Cache는 집계가 계산되는 동안 필드의 값을 메모리에 보관한다

### 5.1.3 실습 데이터 살펴보기
- 스냅숏 목록 확인시 apache-web-log 스냅숏 그룹 내부에는 다음과 같이 2개의 스냅숏이 존재한다.
~~~
curl -XGET 'http://localhost:9200/_snapshot/apache-web-log/_all?pretty'

{
    "snapshots": [
        {
            "snapshot": "default",
            "uuid": "yzmzEx6uSMS55j60z4buBA",
            "repository": "apache-web-log",
            "version_id": 6040399,
            "version": "6.4.3",
            "indices": [
                "apache-web-log"
            ],
            "data_streams": [],
            "include_global_state": false,
            "state": "SUCCESS",
            "start_time": "2019-03-23T16:03:50.351Z",
            "start_time_in_millis": 1553357030351,
            "end_time": "2019-03-23T16:03:50.604Z",
            "end_time_in_millis": 1553357030604,
            "duration_in_millis": 253,
            "failures": [],
            "shards": {
                "total": 5,
                "failed": 0,
                "successful": 5
            },
            "feature_states": []
        },
        {
            "snapshot": "applied-mapping",
            "uuid": "SgXhqApiSHiauC6fbjSHMw",
            "repository": "apache-web-log",
            "version_id": 6040399,
            "version": "6.4.3",
            "indices": [
                "apache-web-log-applied-mapping"
            ],
            "data_streams": [],
            "include_global_state": false,
            "state": "SUCCESS",
            "start_time": "2019-03-23T16:05:46.038Z",
            "start_time_in_millis": 1553357146038,
            "end_time": "2019-03-23T16:05:46.364Z",
            "end_time_in_millis": 1553357146364,
            "duration_in_millis": 326,
            "failures": [],
            "shards": {
                "total": 5,
                "failed": 0,
                "successful": 5
            },
            "feature_states": []
        }
    ],
    "total": 2,
    "remaining": 0
}
~~~
- 여기서는 이중에 default 스냅숏을 복구하겠다 터미널에서 다음 명령을 실행한다.

~~~
curl -XPOST 'http://localhost:9200/_snapshot/apache-web-log/default/_restore'

결과

GET /_cat/indices/apache*?v&pretty

health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   apache-web-log zkKZfOw6RaiFKPM1iMCbkQ   5   1      10001            0      8.8mb          8.8mb
~~~
- 인덱스가 잘 생성되었다면 10001개의 로그 존재
- 지역별 사용자 접속 수 쿼리

~~~
GET /apach-web/log/_search?size=0
{
  "query": {
    "match_all": {}
  },
  "aggs" : {
    "region_count" : {
      "terms": {
        "field": "geoip.region_name.keyword",
        "size": 20
      }
    }
  }
}
~~~
- 집계에서 사용하는 필드 중 문자열 형태의 필드를 사용한다면 Keyword 타입으로 지정해야 한다.
- Keyword 타입은 Text 타입과 달리 분석 과정을 수행하지 않기 떄문에 집계 성능이 향상된다. 예제에서는 geoip.region_name.keyword와 같이 지역명에 대해 Keyword 타입을 지정했다.
- 집계 결과
~~~
{
  "took": 131,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
- 집계를 사용하면 필드를 그룹으로 묶고 통계 결과 도출 가능

### 5.1.4 Aggregation API 이해하기
- 필드의 값을 더하거나 평균을 내는 등 검색 쿼리로 반환된 데이터를 집계하는 경우가 많다
- 검색 쿼리의 결과 집계는 기존 검색 쿼리에 집계 구문 추가하는 방식으로 수행
~~~
{
  "query" : {...},
  "aggs" : {...}
}
~~~
- 엘라스틱서치는 집계 시 문서를 평가한 후 기준에 만족하는 문서들을 하나로 그룹화
- 집계가 끝나면 버킷 목록에 속한 문서의 집합이 출력된다.
~~~
...생략...
"aggregations": {
    "region_count": {
      "doc_count_error_upper_bound": 39,
      "sum_other_doc_count": 3548,
      "buckets": [
        {
          "key": "California",
          "doc_count": 756
        },
        {
          "key": "Texas",
          "doc_count": 588
        },
        {
          "key": "Virginia",
          "doc_count": 424
        },
        {
          "key": "Pennsylvania",
          "doc_count": 355
        },
        {
          "key": "Washington",
          "doc_count": 273
        }
        ... (생략) ...,       
      ]
    }
  }
}
~~~
- 버킷집계 : 쿼리 결과로 도출된 도큐먼트 집합에 대해 특정 기준으로 나눈 다음 너눠진 도큐먼트들에 대한 산술 연산을 수행한다. 이때 나눠진 도큐먼트들의 모음들이 각 버킷에 해당된다.
- 메트릭 집계 : 쿼리 결과로 도출된 도큐먼트 집합에서 필드의 값을 더하거나 평균을 내는 등의 산술 연산을 수행한다.
- 파이프라인 집계 : 다른 집계 또는 관련 메트릭 연산의 결과를 집계한다.
- 행렬 집계 : 버킷 대상이 되는 도큐먼트의 여러 필드에서 추출한 값으로 행렬 연산을 수행한다. 이를 토대로 다양한 통계정보를 제공하고 있으나, 아직은 공식적인 집계 연산으로 제공되지 않고 실험적인 기능으로 제공되기 때문에 사용시 주의

- 엘라스틱서치는 집계를 중첩해 사용 가능. 하위 집계가 상위 집계의 버킷을 다시 집계하는 방식
- ex) 상위 집계에서 date_histogram 집계로 일자별로 집계한 후 그 결과를 메트릭 집계로 다시 합산해 결과 도출 가능.
- 중첩 횟수에 제한은 없으나 중첩할수록 성능은 하락하므로 주의
~~~
참고
현재 엘라스틱서치는 버킷, 메트릭, 파이프라인, 행렬 총 4가지 방식의 집계 연산을 지원한다 그러나 행렬은 실험적이므로 안다룰 예정
~~~
- <b>집계 구문의 구조</b>
  - 문법적 구조를 이해해야 한다. 
~~~
"aggregation" : {
  "<aggregation_name>" : {
    "<aggregation_type>" : {
      <aggregation_body>
    }
    [, "meta" : { [<meta_data_body>] }]?
    [, "aggregations" : { [<sub_aggregation>]+ }]?
  }
 [,"<aggregation_name_2>" : {...} ]*
}
~~~
- 
  - 데이터 집계시 "aggregations" 단어 명시(aggs로 줄여서 쓰는 것도 가능)
  - aggregation_name에는 하위 집계의 이름을 기입. 이 이름은 집계의 결과 출력에 사용. 사용자가 적당한 이름 지어야 한다.
  - aggregation_type은 집계의 유형 적기 (terms, date_histogram, sum)
  - aggregation_body에는 앞서 지정한 aggregation_type에 맞춰 내용을 작성
  - meta필드를 사용하거나 aggregations를 중첩할 수 있는데, 중첩의 경우 같은 레벨(aggregation_name2)에 또 다른 집계를 정의하는 것도 가능.
  - 단 같은 레벨에 집계를 정의할 때는 부수적인 성격의 집계만 정의 가능.
- <b>집계 영역</b>
  - 집계와 함꼐 질의 사용. 집계를 질의와 함께 수행하면 질의의 결과 영역 안에서 집계가 수행
  - 질의를 통해 반환된 문서들의 집합 내에서 집계를 수행하게 된다.
~~~
{
  "query" : { ---- query : 질의를 수행한다. 하위의 필터 조건에 의해 명시된 필드와 값이 일치하는 문서만 반환. 
    "constant_score" : {
      "filter": {
        "match" : <필드조건>
      }
    }
  },
  "aggs" : { ---- aggs : 질의를 통해 반환받은 문서들의 집합 내에서 집계를 수행한다.
    "<집계이름>" : {
      "<집계타입>" : {
        "field" : "<필드명>"
      }
    }
  }
}
~~~
- 
  - 만약 질의가 생략된다면 내부적으로 match_all 쿼리로 수행되어 전체 문서에 대해 집계가 수행된다.
~~~
{
  "size" : 0, ---- size : 질의가 명시돼 있지 않기 때문에 내부적으로는 match_all이 수행되고 size가 0이기 때문에 결과 집합에 문서들 도한 존재하지 않는다. 즉 문서 결과 출력 x
  "aggs" : { ---- aggs : 결과 문서가 출력되지 않더라도 실제 검색된 문서의 대상 범위가 전체 문서이기 때문에 집계는 전체 문서에 대해 수행
    "<집계 이름>" : {
      "<집계 타입>" : {
        "field" : "<필드명>"
      }
    }
  }
}
~~~
-
  - 한 번의 집계를 통해 질의에 해당하는 문서들 내에서도 집계를 수행하고 전체 문서에 대해서도 집계를 수행해야 하는 경우에는 글로벌 버킷을 사용하면 질의 내에서도 전체 문서를 대상으로 집계 수행 가능

~~~
{
  "query" : {
    "constant_score" : {
      "filter" : {
        "match" : <필드 조건>
      } 
    }
  },
  "aggs" : {
    "<집계 이름>": {
      "<집계 타입>" : { ---- 일반 버킷 : 질의 영역 내에서만 집계를 수행 
        "field" : "<필드명>"
      }
    },
    "<집계 이름>" : {
      "global" : {}, ----  글로벌 버킷 : 전체 문서를 대상으로 집계를 수행
      "aggs" : {
        "<집계 이름>": {
          "<집계 타입>" : { 
            "field" : "<필드명>"
          }
        }
      }
    }
  }
}
~~~

## 5.2 메트릭 집계
- 메트릭 집계를 사용하면 특정 필드에 대해 합이나 평균을 계산하거나 다른 집계와 중첩해서 결과에 대해 특정 필드의 _score 값에 따라 정렬을 수행하거나 자리 정보를 통해 범위 계산을 하는 등의 다양한 집계를 수행할 수 있다.
- 정수 또는 실수와 같이 숫자 연산을 할 수 있는 값들에 대한 집계를 수행한다.
- 일반적으로 필드 데이터를 사용해 집계가 이뤄지지만 스크립트를 사용해 조금 더 유연하게 집계 수행 가능
- 단일 숫자 메트릭 집계와 다중 숫자 메트릭 집계로 나뉜다. 
- 단일 : 결과값이 하나 -> sum, avg.
- 다중 : 결과값 여러개 -> stats, geo_bounds등
- 메트릭 집계를 실습하기 위해 apache-web-log 인덱스의 bytes 필드를 주로 사용.
- bytes 필드는 아파치 웹 서버로 수신된 데이터량을 의미하는 필드로서 숫자형으로 돼 있기 때문에 각종 집계 연산을 수행하는데 유용하다.
- 예제에 사용된 쿼리는 전부 전체 데이터에 대해 집계 수행한다. 전체 데이터에 대한 쿼리라면 "query"부분을 생략 가능.
- 집계 결과가 아닌 검색 결과의 내용을 볼 필요가 없는 경우에는 size를 0으로 지정해서 검색 결과가 반환되지 않게도 가능
- 예제에서는 불필요한 결과는 노출 x

~~~
GET /apache-web-log/_search?size=0 --- 집계된 문서들의 데이터는 불필요하므로 size값을 0으로 지정해 반환 받지 않음
{
  "aggs" : { --- 집계 수행, aggregation 또는 aggs 가능
    "<집계 이름>" : { --- 집계에 대한 이름. 하위 쿼리 또는 여러 쿼리를 함께 사용할 때 구별하는 용도로 사용
      "<집계 타입>" : { --- 합계, 평균, 시계열 등의 집계 타입을 명시
        "field" : "<필드명>" --- 집계의 대상이 되는 필드 명시
      }
    }
  }
}
~~~
- ex) 집계 결과 의 공통적 구조 보기

~~~
  {
      "took": 1, --- 엘라스틱서치가 검색을 실행하는데 소요된 시간(ms)
      "time_out": false, --- 검색 시간이 초과됐는지 여부
      "_shard": { --- 검색에 영향받은 샤드에 대한 정보
        "total": 5, --- 검색에 영향받은 샤드의 총 개수
        "successful": 5, --- 검색 요청에 대한 처리를 정상 수행한 샤드 수
        "skipped": 0, --- 검색 요청에 대한 처리를 건너뛴 샤드 수
        "failed": 0, --- 검색 요청에 대한 처리를 실패한 샤드 수
      },
      "hits": { --- 검색 결과
        "total": 200, --- 검색 기준과 일치하는 총 문서 수
         "max_score": 0, --- 검색 결과에 포함된 문서의 최대 스코어 값
         "hits": [ ] --- 검색 결과 문서들의 배열(기본적으로 10개의 문서를 반환)
      },
     "aggreations": { --- 집계 결과
       "<집계 이름>": { --- 검색을 요청할 때 지정한 집계의 이름
         <집계 결과> --- 검색을 요청할 때 지정한 집계 타입에 따른 결과
        }
      }
    }
~~~

### 5.2.1 합산 집계
- 합산 집계는 단일 숫자 메트릭 집계에 해당한다. 해당 서버로 총 얼마만큼 데이터가 유입됐는지 집계해보자
~~~
GET /apache-web-log/_search?size=0
{
  "aggs" : {
    "total_bytes" : {
      "sum" : {
        "field" : "bytes"
      }
    } 
  }
}
~~~
- 쿼리를 수행하면 집계 결과 반환. 이를 통해 현재 서버로 총 얼마만큼의 데이터가 유입됐는지 확인 가능
~~~
{
  ...생략 ...
  "aggregations" : {
    "total_bytes" : {
      "value" : 2747282505
    }
  }
}
~~~
- 이번에는 filter 기능을 사용해 특정 지역에서 유입된 데이터의 합을 계산해보자

~~~
GET /apache-web-log/_search?size=0
{
  "query" : { --- 쿼리 컨텍스트를 의미
    "constant"_score" : { --- 필터에 해당하는 문서들에 대해 동일한 스코어를 부여
      "filter" : { --- 필터 컨텍스트를 의미
        "match" : {"geoip.city_name" : "Paris" } --- geoip.citi_name이 Paris인 문서 검색
      }
    }
  },
  "aggs" : {
    "total_bytes" : {
      "sum" : {
        "field" : "bytes"
      }
    }
  }
}
~~~
- 결과
~~~
{
  ...생략 ...
  "aggregations" : {
    "total_bytes" : {
      "value" : 428964
    }
  }
}
~~~
- 필터 쿼리를 사용하면 위와 같이 파리에서 유입된 데이터의 총량 확인 가능.
- 유입되는 데이터가 많아질수록 데이터의 크기가 커질 것이기 때문에 나중에는 데이터 총략이 커질 수 있다.
- 이럴때 KB, MB, GB로 보고싶을 경우? -> script활용
~~~
GET /apache-web-log/_search?size=0
{
  "query" : { --- 쿼리 컨텍스트를 의미
    "constant"_score" : { --- 필터에 해당하는 문서들에 대해 동일한 스코어를 부여
      "filter" : { --- 필터 컨텍스트를 의미
        "match" : {"geoip.city_name" : "Paris" } --- geoip.citi_name이 Paris인 문서 검색
      }
    }
  },
  "aggs" : {
    "total_bytes" : {
      "sum" : {
        "script" :  { --- 스크립트 컨텍스트를 의미, 6.x부터는 그루비가 아닌 페인리스를 기본 언어로 사용
          "lang" : "painless", --- 페인리스 언어를 사용하는게 default(여기서는 언어 확인 위해 명시)
          "source" : "doc.bytes.value" --- 필드를 사용하려면 doc객체의 bytes를 속성 변수로 사용해야 하고, 값을 얻기 위해 bytes 객체의 value 속성 변수를 사용
        }
      }
    }
  }
}
~~~

- 결과 
~~~
{
  ...생략 ...
  "aggregations" : {
    "total_bytes" : {
      "value" : 428964
    }
  }
}
~~~

- script를 사용하면 기존에 합산만 수행했던 것에서 더 나아가 다양한 연산 수행 가능.
- KB로 나타내기 위해 계산
~~~
GET /apache-web-log/_search?size=0
{
  "query" : { --- 쿼리 컨텍스트를 의미
    "constant"_score" : { --- 필터에 해당하는 문서들에 대해 동일한 스코어를 부여
      "filter" : { --- 필터 컨텍스트를 의미
        "match" : {"geoip.city_name" : "Paris" } --- geoip.citi_name이 Paris인 문서 검색
      }
    }
  },
  "aggs" : {
    "total_bytes" : {
      "sum" : {
        "script" :  { --- 스크립트 컨텍스트를 의미, 6.x부터는 그루비가 아닌 페인리스를 기본 언어로 사용
          "lang" : "painless", --- 페인리스 언어를 사용하는게 default(여기서는 언어 확인 위해 명시)
          "source" : "doc.bytes.value / params.divide_value" --- byte 필드의 값을 script내의 params(변수와 같은 의미) 에 명시한 값으로 나눔
          "params": { --- script 내에서 사용할 파라미터 값들을 정의
            "divide_value" : 1000 --- 파라미터의 값으로 1000 대입.
          } 
        }
      }
    }
  }
}
~~~
- 결과 
~~~
{
  ...생략 ...
  "aggregations" : {
    "total_bytes" : {
      "value" : 422
    }
  }
}
~~~
- 왜 428이 아닐까? -> 1000으로 나누는 모든 합산 값에 대해 나누기가 아니라 각 문서의 개별적인 값을 1000으로 나눈 것이므로 1000보다 작은 문서는 0이 됨
- 이 문제 해결 위해 정수가 아닌 실수로 계산해서 소수점까지 합산해야 한다. 

~~~
GET /apache-web-log/_search?size=0
{
  "query" : { --- 쿼리 컨텍스트를 의미
    "constant"_score" : { --- 필터에 해당하는 문서들에 대해 동일한 스코어를 부여
      "filter" : { --- 필터 컨텍스트를 의미
        "match" : {"geoip.city_name" : "Paris" } --- geoip.citi_name이 Paris인 문서 검색
      }
    }
  },
  "aggs" : {
    "total_bytes" : {
      "sum" : {
        "script" :  { --- 스크립트 컨텍스트를 의미, 6.x부터는 그루비가 아닌 페인리스를 기본 언어로 사용
          "lang" : "painless", --- 페인리스 언어를 사용하는게 default(여기서는 언어 확인 위해 명시)
          "source" : "doc.bytes.value / (double)params.divide_value" --- byte 필드의 값을 script내의 params(변수와 같은 의미) 에 명시한 값으로 나눔 double로 형변환
          "params": { --- script 내에서 사용할 파라미터 값들을 정의
            "divide_value" : 1000 --- 파라미터의 값으로 1000 대입.
          } 
        }
      }
    }
  }
}
~~~
- 결과
~~~
{
  ...생략 ...
  "aggregations" : {
    "total_bytes" : {
      "value" : 428.9639999999994
    }
  }
}
~~~
- 스크립트 사용시 더 다양한 연산 추가적으로 수행하므로 유용

### 5.2.2 평균 집계