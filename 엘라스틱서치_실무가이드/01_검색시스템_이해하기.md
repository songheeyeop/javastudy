# 01 검색 시스템 이해하기
- 대표적인 검색 엔진인 엘라스틱 서치와 데이터베이스의 차이를 알아본다.
- 그리고 엘라스틱 서치가 가지는 강점 및 검색시스템과 관련된 기본적인 개념을 알아봄

## 1.1 검색 시스템의 이해
- 검색엔진과 데이터 베이스의 비교, 먼저 검색 시스템의 등장 배경을 살펴보고 검색 시스템의 특징과 필요성을 공부한다.

### 1.1.1 검색시스템이란?
- 사용자가 원하는 검색어에 대한 결과를 제공하는 네이버나 구글에서 가장 많이 사용하는 검색 서비스
- 검색엔진, 검색 시스템, 검색 서비스 등의 용어로 쓰인다.(모두 비슷하다고 생각하나 그렇지 않다.)
  - 검색엔진 : 광활한 웹에서 정보를 수집해 검색 결과를 제공하는 프로그램. 검색엔진은 검색 결과로 제공되는 데이터의 특성에 따라 구현 형태가 각각 달라진다. 야후는 디렉토리 기반의 검색 결과를 세계 최초로 제공했다. 이를 바탕으로 요즘에는 뉴스, 블로그, 카페 등 대범주에 따른 카테고리별 검색 결과를 대부분의 검색 업체에서 제공하고 있다.
  - 검색시스템 : 대용량 데이터를 기반으로 신뢰성 있는 검색 결과를 제공하기 위해 검색 엔진을 기반으로 구축된 시스템을 통칭하는 용어. 수집기를 이용해 방대한 데이터를 수집하고 이를 다수의 검색엔진을 이용해 색인하고 검색 결과를 UI로 제공. 시스템 내부의 정책에 따라 관련도가 높은 문서를 검색 결과의 상위에 배치할 수 있을 뿐더러 특정 필드나 문서에 가중치를 둬서 검색의 정확도를 높일 수도 있다.
  - 검색 서비스 :  검색 엔진을 기반으로 구축된 검색 시스템을 활용해 검색 결과를 서비스로 제공
- 검색 서비스 > 검색시스템 > 검색엔진
- 엘라스틱 서치는 검색엔진이며 이 책에서는 검색 서비스를 제공하기 위해 엘라스틱 서치를 이용해 검색 시스템을 구축할 것이다.

### 1.1.2 검색 시스템의 구성 요소
- 검색 시스템의 기본 구조는 일반적으로 정보를 수집하는 수집기, 수집한 데이터를 저장하는 스토리지, 수집한 테이터를 검색에 적절한 형태로 변환하는 색인기, 색인된 데이터에서 일치하는 문서를 찾는 검색기로 구성
- 수집기
  - 웹사이트, 블로그, 카페등 웹에서 필요한 정보를 수집하는 프로그램. 크롤러, 스파이더, 웜, 웹로봇 등으로도 불린다.
  - 파일, 데이터베이스, 웹페이지 등 웹상의 대부분의 정보가 수집 대상이다.
  - 파일의 경우 수집기가 파일 명, 파일 내용, 파일 경로 등의 정보를 수집하고 저장하면 검색엔진이 저장된 정보를 ㄱ머색하고 사용자 질의에 답한다.
- 스토리지
  - 데이터베이스에서 데이터를 저장하는 물리적인 저장소다. 검색엔진은 색인된 데이터를 스토리지에 보관한다.
- 색인기
  - 검색 엔진이 수집한 정보에서 사용자 질의와 일치하는 정보를 찾으려면 수집된 데이터를 검색 가능한 구조로 가공하고 저장해야 한다.
  - 그 역할을 하는 것이 색인기
  - 색인기는 다양한 형태소 분석기를 조합해 정보에서 의미가 있는 용어를 추출하고 검색에 유리한 역색인 구조로 데이터를 저장한다.
- 검색기
  - 검색기는 사용자 질의를 입력받아 색인기에서 저장한 역색인 구조에서 일치하는 문서를 찾아 결과로 반환한다.
  - 질의와 문서가 일치하는지도 유사도 기반의 검색 순위 알고리즘으로 판단한다.
  - 검색기 또한 색인기와 마찬가지로 형태소 분석기를 이용해 사용자 질의에서 유의미한 용어를 추출해 검색한다.
  - 사용하는 형태소 분석기에 따라 검색 품질이 달라진다.

### 1.1.3 관계형 데이터에스와의 차이점
- RDBMS와 검색엔진 모두 질의와 일치하는 데이터를 찾아 사용자에게 제공한다는 점에서 유사점이 많지만, 관계형 데이트베이스로 검색기능을 제공하는데는 많은 문제점이 있다.
- RDBMS의 한계
  - 데이터베이스는 데이터를 통합 관리하는 데이터의 집합.
  - 모든 데이터는 중복을 제거하고, 정형 데이터로 구조화해 행과 열로 구성된 테이블에 저장된다.
  - SQL문을 이용해 원하는 정보의 검색이 가능한데, 텍스트 매칭을 통한 단순한 검색만 가능하다.
  - 텍스트를 여러 단어로 변형하거나 여러 개의 동의어나 유의어를 활용한 검색은 불가능하다.
- 검색엔진
  - 데이터베이스에서는 불가능한 비정형 데이터를 색인하고 검색할 수 있다.
  - 형태소 분석을 통해 사람이 구사하는 자연어의 처리 가능
  - 역색인 구조를 바탕으로 빠른 검색 속도 보장
- 표 1.1 엘라스틱서치와 관계형 데이터베이스 비교

|엘라스틱서치|관계형 데이터베이스|
|----|-----|
|인덱스|데이터베이스|
|샤드|파티션|
|타입|테이블|
|문서|행|
|필드|열|
|매핑|스키마|
|Query DSL|SQL|

- 엘라스틱 서치의 인덱스는 관계형 데이터베이스의 데이터베이스와 비슷한 문서의 모음을 뜻한다.
- 엘라스틱 서치의 타입은 데이터베이스의 테이블과 같은 역할을 한다.
- 6.0 이하 버전에서는 하나의 인덱스 내부에 기능에 따라 데이터를 분류하고 여러 개의 타입을 만들어 사용했지만 현재는 하나의 인덱스에 하나의 타입만을 구성하도록 바뀌었다.
- 엘라스틱 서치는 하나의 행을 문서라고 부르며, 해당 문서는 데이터 베이스의 한 행을 의미한다.
- 엘라스틱 서치의 매핑은 필드의 구조와 제약조건에 대한 명세를 말하며 이에 대응하는 관계형 데이터베이스의 개념을 스키마라 한다.
- 엘라스틱 서치의 인덱스는 문서의 모음, 관계형 데이터베이스에서의 인덱스는 WHERE 절의 쿼리와 JOIN을 빠르게 만드는 보조 데이터 도구로 사용
- 표 1.2 추가, 검색, 삭제, 수정 기능 비교

|엘라스틱 서치에서 사용하는 HTTP 메소드 | 기능 | 데이터베이스 질의 문법|
|-------------------|---|---|
|GET|데이터 조회 | SELECT|
|PUT|데이터 생성| INSERT|
|POST|인덱스 업데이트, 데이터 조회 | UPDATE, SELECT|
|DELETE|데이터 삭제|DELETE|
|HEAD|인덱스의 정보 확인| |

- 엘라스틱 서치는 기본적으로 HTTP를 통해 JSON 형식의 RESTful API를 이용한다.
- RESTful API는 HTTP 헤더와 URL만을 사용해 다양한 형태의 요청을 할 수 있는 HTTP 프로토콜을 최대한 활용하도록 고안된 아키텍처다.
- 엘라스틱 서치를 사용하기 위한 간단한 API 요청 구조다.
  - <strong> curl -X(메소드) http://host:port/(인덱스)/(타입)/(문서 id) -d '{json data}'</strong>
  - curl에서 제공하는 -x 옵션을 사용하면 GET, POST, PUT, DELETE, HEAD와 같은 메소드 설정할 수 있다.
  - GET : 문서의 현재 상태 정보 얻음
  - POST : 특정 상태를 생성하거나 문서를 조회함
  - PUT : 문서 업데이트
  - DELETE : 문서 삭제
  - HEAD : 특정 문서의 정보 유무를 확인
- 리눅스에서는 엘라스틱서치 서버의 정보를 다음 명령으로 확인할 수 있다.
  - <strong>crul -XGET http://localhost:9200 </strong>
- 위 명령어 실행 결과
~~~
  {
  "name" : "95fea20542d3",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "dSX4qGEsTOmSeQeAkYUynA",
  "version" : {
    "number" : "8.0.0",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "1b6a7ece17463df5ff54a3e1302d825889aa1161",
    "build_date" : "2022-02-03T16:47:57.507843096Z",
    "build_snapshot" : false,
    "lucene_version" : "9.0.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
~~~
- 표 1.3 사용자 정보 데이터

|ID|Name|Location|Gender|Date|
|---|---|---|---|---|
|1|가마돈|서울|남|2018-05-12|
|2|로이드|도쿄|여|2018-05-11|
- 데이터베이스에서는 '가마돈'이라는 사용자의 데이터를 조회할 경우

~~~ 
SELECT * FROM USER WHERE Name like '%가마돈%' 
~~~
- 엘라스틱서치에서는 검색엔진이 제공하는 Search API를 사용해야 한다.
~~~
  GET http://localhost:9200/user/_search?q=Name:가마돈
~~~

~~~
{
  "ID" : 1,
  "Name" : "가마돈",
  "Location" : "서울", 
  "Gender" : :"남",
  "Date" : "2018-05-12"
}
~~~

- 만약 가마돈이 영어로 되어 있을 경우 RDBMS는 Where name like '%Garmadon%' 이라고 대소문자를 명확하게 입력해야 검색이 가능하고 모든 문자열이 소문자이거나 대문자인 경우에는 해당하는 데이터를 찾을 수 없을 것이다.
  - ->?? https://zetawiki.com/wiki/MySQL_%EB%8C%80%EC%86%8C%EB%AC%B8%EC%9E%90_%EA%B5%AC%EB%B3%84 아 원래 데이터 다 조회 되는데!!
- 엘라스틱 서치는 좀 더 유연하다 역색인되는 데이터 전체를 정책에 따라 소문자 혹은 대문자로 생성하고 쿼리가 들어오는 필터를 색인 검색 시간과 동일하게 지정한다면 해당하는 쿼리에 어떤한 문자열(GARMADON, Garmadon, GarMaDon)도 검색 가능
- 또한 구조화되지 않은 비정형 데이터도 검색이 가능해진다. 데이터베이스는 스키마를 미리 정의해야만 데이터 저장과 조회가 가능한 반면, 엘라스틱서치는 구조화되지 않은 데이터까지 스스로 분석해 자동으로 필드를 생성하고 저장한다.

## 1.2 검색 시스템과 엘라스틱 서치
- 대량의 데이터를 빠르게 검색하기 위해 NoSQL을 많이 사용한다.
- 엘라스틱 서치도 NoSQL의 일종으로서 분류가 가능하고 분산 처리를 통해 실시간에 준하는 빠른 검색이 가능하다.
- 기존 데이터베이스로는 처리하기 어려운 대량의 비정형 데이터도 검색할 수 있으며, 전문 검색과 구조 검색 모두를 지원한다.
- 기본적으로는 검색엔진이지만 MongoDB나 Hbase처럼 대요량 스토리지로도 활용할 수 있다.

### 1.2.1 엘라스틱서치가 강력한 이유
- 오픈소스 검색엔진
  - 아파치 재단의 루씬(Lucene)을 기반으로 개발된 오픈소스 검색엔진이다.
  - 많이 사용하므로 버그도 빠르게 해결되고, 활성화도 많이 되어있다.
- 전문 검색
  - PostgreSQL, MongoDB같은 대부분의 데이터베이스는 기본 쿼리 및 색인 구조의 한계로 인해 기본적인 텍스트 검색 기능만 제공한다.
  - 엘라스틱서치는 좀 더 고차원적인 전문검색(Full Text)이 가능하다.
    - 전문 검색이란 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것을 말한다.
  - 엘라스틱서치는 다양한 기능별, 언어별 플러그인을 조합해 빠르게 검색할 수 있다.
- 통계 분석
  - 엘라스틱서치와 키바나를 연결하면 실시간으로 쌓이는 로그를 시각화하고 분석할 수 있다.
- 스키마리스
  - 데이터베이스는 스키마라는 구조에 따라 데이터를 적합한 형태로 변경해서 저장하고 관리한다. 반면 엘라스틱서치는 정형화되지 않은 다양한 형태의 문서로 자동으로 색인하고 검색할 수 있다.
- RESTful API
  - 엘라스틱서치는 HTTP기반의 RESTful API를 지원하고 요청뿐 아니라 응답에도 JSON 형식을 사용해 개발 언어, 운영체제, 시스템에 관계없이 이기종 플랫폼에서도 이용 가능하다.
- 멀티테넌시
  - 서로 상이한 인덱스일지라도 검색할 필드명만 같으면 여러 개의 인덱스를 한번에 조회할 수 있다. 이를 이용해 멀티테넌시 기능을 제공할 수 있다.
    - https://sungks.tistory.com/161
- Document-Oriented
  - 여러 계층의 데이터를 JSON 형식의 구조화된 문서로 인덱스에 저장할 수 있다. 계층 구조로 문서도 한번의 쿼리로 쉽게 조회할 수 있다.
- 역색인
  - 루씬 기반의 검색엔진이므로 역색인을 지원한다. 다른 NoSQL 대비 엘라스틱서치의 매우 큰 장점이라 할 수 있다.
  - 역색인이란 종이책의 마지막 페이지에서 제공하는 색인 페이지와 비슷하게 제공되는 특수한 데이터 구조다.
  - 표 1.5 역색인된 단어와 문서 번호와의 관계

|단어 | 문서번호|
|---|---|
|엘라스틱서치| 1|
|검색엔진| 1,2|
|역색인| 2,3|
|데이터베이스| 3|
-
  - '검색엔진'이란 단어가 포함된 모든 문서를 찾아야한다고 하자. 일반적으로는 처음부터 끝까지 모든 문서를 읽어야만 원하는 결과를 얻을 수 있을 것이다. 하지만 역색인 구조는 해당 단어만 찾으면 단어가 포함된 모든 문서의 위치를 알 수 있기 때문에 빠르게 찾을 수 있다.
- 확장성과 기용성
  - 10억개의 문서를 색인한다고 가정해보자. 모든 문서를 색인하는 데 막대한 비용과 시간이 든다.
  - 엘라스틱서치를 분산 구성해서 확장한다면 대량의 문서를 좀 더 효율적으로 처리할수 있다.
  - 분산 환경에서 데이터는 샤드라는 작은 단위로 나뉘어 제공되며, 인덱스를 만들 때마다 샤드의 수를 조절할 수 있으므로 데이터의 종류와 성격에 따라 데이터를 분산해서 빠르게 처리할 수 있다.

### 1.2.2 엘라스틱 서치의 약점
1. 실시간이 아니다. 색인된 데이터는 통상적으로 1초 뒤에나 검색이 가능해진다.(준 실시간)
2. 트랜잭션과 롤백 기능을 제공하지 않는다. (최악의 경우 데이터 손실 위험)
3. 데이터의 업데이트를 제공하지 않는다. (단순 업데이트에 비해서는 상대적으로 많은 비용 발생) 그러나 불변적이라는 이점을 취할 수 있다.

## 1.3 실습 환경 구축
- 엘라스틱서치 노드 하나만으로 구성된 클러스터를 싱글 모드 또는 테스트 모드라고 부름. 이것은 서비스 목적이 아니라 테스트 목적이므로 실무 사용 x
- 엘라스틱서치로 실제 서비스를 운영할 때는 최소 3개 이상의 물리적인 노드로 클러스터를 구축하는 것이 좋다.

### 1.3.1 엘라스틱서치 설치
- 엘라스틱서치는 자바 언어로 개발된 프로그램.
- 자바 설치
  - 1.8 이상 추천
- 엘라스틱서치 설치
  - 실행 -> elasticsearch.bat실행
  - 밑 소스 정보가 뜨면 실행 완료
  - 요새는 ssl이 기본 설정 true로 되어 있으므로 elasticsearch.yml 파일에서 xpack.security.enabled: true를
  - xpack.security.enabled: false로 변경시켜야 한다.
~~~
[2022-03-04T18:11:53,308][INFO ][o.e.h.AbstractHttpServerTransport] [DESKTOP-72G75BD] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}
[2022-03-04T18:11:53,309][INFO ][o.e.n.Node               ] [DESKTOP-72G75BD] started
~~~

-
  - 웹 브라우저의 주소창에 localhost:9200 입력시 클러스터 이름과 버전 정보가 나타난다

~~~
{
  "name" : "DESKTOP-72G75BD",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "UqqKAvqwQ7-giJkDKqwVqA",
  "version" : {
    "number" : "8.0.1",
    "build_flavor" : "default",
    "build_type" : "zip",
    "build_hash" : "801d9ccc7c2ee0f2cb121bbe22ab5af77a902372",
    "build_date" : "2022-02-24T13:55:40.601285296Z",
    "build_snapshot" : false,
    "lucene_version" : "9.0.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
~~~
-
  - cluster_name은 엘라스틱 서치의 클러스터를 구분하는 중요한속성.
  - 아무런 설정을 하지 않으면 elasticsearch로 자동 설정되는데 충돌할 수 있으므로 반드시 수정해야 한다.
  - 엘라스틱서치 콘솔 또는 명령 프롬프트 창에서 Ctrl+C를 눌러 프로세스를 종료할 수 있다. 종룔시 노드가 종료됐다는 메시지가 출력된다.

~~~
[2022-03-04T18:24:22,590][INFO ][o.e.n.Node               ] [DESKTOP-72G75BD] closed
~~~

### 실습을 위한 엘라스틱서치 준비
- 7장에서 제공되는 한글 검색 확장 기능을 실습하려면 별도로 제공되는 커스텀 플러그인을 반드시 설치해야만 한다.
- 커스텀 플러그인을 설치하기 위해서는 엘라스틱서치 보안 정책에 의해 사용하는 버전마다 빌드해야하지만 플로그인 개발 방식에 익숙하지 않은 경우 빌드하는 데 어려움을 겪을 수도 있다.
- 6.3.4를 권장한다 플러그인 실습 예제를 위해..
- elasticsearch.yml 파일에서는 클러스터 이름, 노드 이름, 로그 경로, 데이터 경로등 다양한 설정을 지정할 수 있다.
- 관심 가져야할 주요 설정 항목
  - cluster.name : 클러스터로 여러 노드를 하나로 묶을 수 있는데, 여기서 클러스터명을 지정할 수 있다. 여기서는 클러스터명으로 javacafe-cluster로 지정
  - node.name : 엘라스틱서치 노드명을 설정한다. 노드명을 지정하지 않으면 엘라스틱서치가 임의의 이름으로 자동으로 부여한다. 여기서는 javacafe-node1로 지정.
  - path.data : 엘라스틱서치의 인덱스 경로 지정한다. 설정하지 않으면 기본적으로 엘라스틱서치 하위의 data 디렉터리에 인덱스가 생성된다.
  - path.log : 엘라스틱서치의 노드와 클러스터에서 생성되는 로그를 저장할 경로를 지정한다. 기본 경로는 /path/to/logs
  - path.repo : 엘라스틱서치 인덱스를 백업하기 위한 스냅숏의 경로를 지정한다. 예제로 제공되는 스냅숏의 경로를 지정하자. 만약 기준 경로가 /es/book_backup 디렉토리라면 다음과 같이 설정
    1. 리눅스 : path.repo : ["/es/book_backup/search_example", "/es/book_backup/agg_example"]
    2. 윈도우 : path.repo : ["C:\\es\\book_backup\\search_example", "C:\\es\\book_backup\\agg_example"]
  - network.host : 특정 IP만 엘라스틱서치에 접근하도록 허용할 수 있다. 선택적으로 IP를 허용해야 할 경우 [1.1.1.1, 2.2.2.2]와 같이 지정하고 모든 IP를 허용한다면 0.0.0.0 지정. IP값으로 127.0.0.1을 설정하면 개발 모드에서 프로덕트 모드로 자동으로 변경된다. 여기서는 IP로 0.0.0.0을 지정
  - http.port : 엘라스틱서치 서버에 접근할 수 있는 HTTP API 호출을 위한 포트 번호를 지정한다. 여기서는 기본인 9200번 사용
  - transport.tcp.port : 엘라스틱서치 클라이언트가 접근할 수 있는 TCP포트다. 여기서는 기본인 9300번 사용
  - discovery.zen.ping.unicast.hosts : 노드가 여러 개인 경우 유니캐스트로 활성화된 다른 서버를 찾는다. 클러스터로 묶이느 노드(서버)의 IP를 지정하면 된다. 예컨대 노드가 2개인 경우 [1.1.1.1, 2.2.2.2]처럼 지정하면 된다. 실습 환경은 단일 노드이기 때문에 따로 설정할 내용은 없다.
  - discovery.zen.minimum_master_nodes : 마스터 노드의 선출 기준이 되는 노드의 수를 지정한다. 실습 환경은 단일 노드이므로 따로 설정하지 않아도 된다.
  - node.master : 마스터 노드로 동작 여부를 지정한다. 예제에서는 마스터 노드와 데이터 노드의 역할을 함께 수행하도록 true로 설정한다.
  - node.data : 데이터 노드로 동작 여부를 지정한다. 예제에서는 마스터 노드와 데이터 노드의 역할을 함께 수행하도록 true로 설정한다.
- 엘라스틱서치가 정상적으로 시작되면 path.repo에서 설정한 물리적인 스냅숏 데이터를 엘라스틱서치로 인식시켜야 한다.
- path.repo에 설정된 두 개의 디렉터리 중에서 먼저 search_example 디렉토리 데이터를 활성화한다.
~~~
  curl -XPUT 'http://localhost:9200/_snapshot/javacafe' -d '{
    "type": "fs",
    "settings": {
        "location": "D:\\elasticsearch-8.0.1\\book_backup\\search_example",
        "compress": true
    }
}'
~~~
- 생성된 논리적인 javacafe 스냅숏
~~~
{
    "snapshots": [
        {
            "snapshot": "movie-search",
            "uuid": "Kz5k4fusS7KBZy55wLeZ0Q",
            "repository": "javacafe",
            "version_id": 6040399,
            "version": "6.4.3",
            "indices": [
                "movie_search"
            ],
            "data_streams": [],
            "include_global_state": false,
            "state": "SUCCESS",
            "start_time": "2019-03-23T16:01:04.910Z",
            "start_time_in_millis": 1553356864910,
            "end_time": "2019-03-23T16:01:05.342Z",
            "end_time_in_millis": 1553356865342,
            "duration_in_millis": 432,
            "failures": [],
            "shards": {
                "total": 5,
                "failed": 0,
                "successful": 5
            },
            "feature_states": []
        }
    ],
    "total": 1,
    "remaining": 0
}
~~~

- 두번째로 agg_example 디렉토리의 데이터를 활성화해보자
- 다음과 같은 명령어를 실행하면 agg_example데이터가 apache-web-log라는 이름의 논리적인 스냅숏으로 생성된다.
~~~
crul -XPUT 'http://localhost:9200/_snapshot/apache-web-log' -d '{
    "type": "fs",
    "settings": {
        "location": "D:\\elasticsearch-8.0.1\\book_backup\\agg_example",
        "compress": true
    }
}'
~~~
- agg_example 데이터가 apache-web-log라는 이름의 논리적인 스냅숏으로 생성된 결과

~~~
{
    "snapshots": [
        {
            "snapshot": "default",
            "uuid": "yzmzEx6uSMS55j60z4buBA",
            "repository": "apache-web-log",
            "version_id": 6040399,
            "version": "6.4.3",
            "indices": [
                "apache-web-log"
            ],
            "data_streams": [],
            "include_global_state": false,
            "state": "SUCCESS",
            "start_time": "2019-03-23T16:03:50.351Z",
            "start_time_in_millis": 1553357030351,
            "end_time": "2019-03-23T16:03:50.604Z",
            "end_time_in_millis": 1553357030604,
            "duration_in_millis": 253,
            "failures": [],
            "shards": {
                "total": 5,
                "failed": 0,
                "successful": 5
            },
            "feature_states": []
        },
        {
            "snapshot": "applied-mapping",
            "uuid": "SgXhqApiSHiauC6fbjSHMw",
            "repository": "apache-web-log",
            "version_id": 6040399,
            "version": "6.4.3",
            "indices": [
                "apache-web-log-applied-mapping"
            ],
            "data_streams": [],
            "include_global_state": false,
            "state": "SUCCESS",
            "start_time": "2019-03-23T16:05:46.038Z",
            "start_time_in_millis": 1553357146038,
            "end_time": "2019-03-23T16:05:46.364Z",
            "end_time_in_millis": 1553357146364,
            "duration_in_millis": 326,
            "failures": [],
            "shards": {
                "total": 5,
                "failed": 0,
                "successful": 5
            },
            "feature_states": []
        }
    ],
    "total": 2,
    "remaining": 0
}
~~~

### 1.3.2 키바나 설치
- 키바나는 엘라스틱에서 제공하는 데이터 시각화 프로그램이다.
- 키바나 이용시 엘라스틱서치에서 색인된 데이터를 검색하거나 문서를 추가하거나 삭제하는 등의 기능을 손쉽게 구현할수 있다.
- 키바나 설치와 실행
  - 키바나는 32비트 및 64비트 아키텍쳐에서 여러 가지 형태로 제공.
  - 키바나 다운로드 페이지에서 최신버전을 내려받는다. 내려받은 파일의 압축을 풀고 config 디렉토리를 연다음 kibana.yml 에 내용 추가하거나 주석을 해제한다.
~~~
elasticsearch.url: "http://localhost:9200"
-> 바뀌어서 elasticsearch.hosts: ["http://localhost:9200"] 
여기 주석 해제?
~~~
- http://localhost:5601 접속후 Dev Tools 클릭
~~~
http://localhost:5601/app/dev_tools#/console
~~~
- curl 명령과 유사하지만 더 간편하게 요청
- 사용할 HTTP 메소드와 인덱스명, API 종류를 적고 바로 아래에 JSON 형태의 문장을 입력한다.
- 작엉하는 요청 내용 유칙
~~~
GET ---1 _search ---2
{ ---3
  "query": {
    "match_all": {}
  }
}
~~~
- 1. 요청 전달 방식이다. GET, POST, PUT, DELETE를 지정할 수 있다. GET은 어떠한 변경 없이 쿼리ㅔ 대한 결과를 반환받는 용도로 사용하고, POST는 변경, PUT은 삽입, DELETE는 삭제 용도로 사용된다. 이는 일반적인 RESTful 방식에 해당하므로 쉽게 이해할 수 있다. 이전 curl 명령에서 -X옵션에 해당한다.
- 2. _search는 검색 쿼리를 의미한다. _search 앞부분에 인덱스를 명시해서 해당 인덱스로만 범위를 한정해서 검색을 수행할 수도 있다. 여기서는 어떠한 인덱스도 지정하지 않았기 때문에 전체 인덱스를 대상으로 검색이 수행된다. size가 기본값 10으로 설정돼 있기 때문에 검색 결과로 10개의 문서만 반환하므로 많은 양의 문서가 색인돼 있더라도 결과가 빠르게 반환된다. curl 명령에서는 -X옵션 다음으로 지정했던 도메인 부분에 해당한다. 키바나를 통해 전달되는 쿼리는 무조건 설정에 지정된 엘라스틱서치로 전달되기 때문에 이 부분은 생략 가능
- 3. 쿼리 본문에 해당하며 여기서는 모든 문서를 검색한다. curl 명령에서는 -d 옵션에 해당한다.
- 데이터 전송 예제
  - movie_kibana_execute 인덱스를 생성해서 문서를 하나 색인하기
  - HTTP 메소드는 PUT으로 설정하고 movie_kibana_execute/_doc/1을 입력
  - 다음 라인에 JSON 본문으로 다음과 같은 코드 입력
~~~
PUT movie_kibana_execute/_doc/1
{
  "message":"helloworld"
}
~~~
- 그림 1.6 엘라스틱서치에서 생성된 1번문서
![색인결과](/image/img.png)

- 이번에는 생성된 문서를 검색해보자. 하단데 다음과 같이 검색 명령어를 추가로 입력한다.
- 그림 1.7
![검색하기](/image/img_1.png)
- 앞으로는 키바나의 Dev Tools를 사용할 것이나, 혹시나 한글 테스트를 원할 경우 postman을 사용해라

### 1.3.3 환경 구축 관련 트러블슈팅
- 테스트를 위해 엘라스틱서치를 설치 후 실행하는 과정에서 다양한 오류들이 발생
- "system call filters failed to install"
  - 이 에러는 엘라스틱서치를 실행할 때 부트스트랩(bootstrap) 과정을 실패했기 때문에 발생하는 문제다.
  - 즉 테스트가 수행될 장비에 시스템콜 필터가 설치되지 않은 경우에 발생한다.
  - 이 문제는 elasticsearch.yml 파일에 bootstrap을 사용하지 않도록 옵션을 추가해 해결할 수 있다.
  - ERROR: [1] bootstrap checks failed
    [1]: system call filters failed to install; check th logs and fix your configuration or disable system call filters at your own risk
  - 추가할 항목은 다음과 같다
  - bootstrap.system_call_filter:false

- "max file description[4096] for elasticsearch process likely too low..."
  - 1번 문제와 마찬가지로 부트스르랩에 실패해서 발생하는 에러다. 이 에러는 리눅스의 파일 오픈 개수를 늘려 해결할 수 있다.
  - ERROR: bootstrap checks failed 
    max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536]
  
    max number of threads [1024] for user [space_home] likely too low, increase to at least [2048]
    
    파일 오픈 개수를 늘리는 작업은 리눅스의 root 계정으로 수행해야 한다. root 계정으로 다음 명령을 실행해 열 수 있는 파일의 최대 개수를 확인하자
  
    ulimit -a

    아마 최대 1024개의 파일을 동시에 열 수 있을 것이다.
    
    ... 

    max locked memory (kbytes, -l) 64

    max memory size (kbytes, -m) unlimited

    open files (-n) 1024

    ...

    다음 명령으로 리눅스에서 동시에 열 수 있는 최대 파일 개수를 늘리자

    echo 262144 > /proc/sys/vm/max_map_count

    엘라스틱 서치에서도 작업이 필요하다. /etc/security/limits.conf에 다음 코드를 추가해 엘라스틱서치를 실행할 계정의 파일 개수를 최대로 설정한다. 다음은 elastic이란 이름의 계정으로 엘라스틱서치를 실행하는 예다.
  
    elastic hard nofile 65536

    elastic soft nofile 65536

    elastic hard nproc 65536

    elastic soft nproc 65536

- "\Common은(는) 예상되지 았습니다."
  - 이 오류는 엘라스틱서치에서 자바의 경로를 찾지 못해서 생기는 오류다. bin폴더의 elasticsearch.bat 혹은 elasticsearch 파일을 열어 맨 위에 자바 경로를 설정하면 된다.
  - set "JAVA_HOME=C:\Program Files\Java\jdk1.8"